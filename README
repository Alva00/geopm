GEOPM - Global Energy Optimization Power Management
===================================================

DISCLAIMER
----------
SEE COPYING FILE FOR LICENSE INFORMATION.

LAST UPDATE
-----------
2015 December 11

Christopher Cantalupo <christopher.m.cantalupo@intel.com> <br>
Steve Sylvester <steve.s.sylvester@intel.com>

WEB PAGE
--------
http://geopm.github.io/geopm

SUMMARY
-------
Global Energy Optimization Power Management (GEOPM) is an extensible power
management framework targeting high performance computing.  The library can be
extended to support new control algorithms and new hardware power management
features.  The GEOPM package provides built in features ranging from static
management of power policy for each individual compute node, to dynamic
coordination of power policy and performance across all of the compute nodes
hosting one MPI job on a portion of a distributed computing system.  The
dynamic coordination is implemented as a hierarchical control system for
scalable communication and decentralized control.  The hierarchical control
system can optimize for various objective functions including maximizing
global application performance within a power bound.  The root of the control
hierarchy tree can communicate through shared memory with the system resource
management daemon to extend the hierarchy above the individual MPI job level
and enable management of system power resources for multiple MPI jobs and
multiple users by the system resource manager.  The geopm package provides the
libgeopm library, the libgeopmpolicy library, the geopmctl application and the
geopmpolicy application.  The libgeopm library can be called within MPI
applications to enable application feedback for informing the control
decisions.  If modification of the target application is not desired then the
geopmctl application can be run concurrently with the target application.  In
this case, target application feedback is inferred by querying the hardware
through Model Specific Registers (MSRs).  With either method (libgeopm or
geopmctl), the control hierarchy tree writes processor power policy through
MSRs to enact policy decisions.  The libgeopmpolicy library is used by a
resource manager to set energy policy control parameters for MPI jobs.  Some
features of libgeopmpolicy are available through the geopmpolicy application
including support for static control.

BUILD REQUIREMENTS
------------------
The geopm package requires a compiler that supports the MPI 2.2 and C++11
standards.  These requirements can be met by using GCC version 4.7 or greater
and installing the openmpi-devel package version 1.7 or greater on RHEL or
SLES Linux.  Additionally building the geopm package requires the json-c
library and the hwloc library. Documentation creation including manpages
further requires the rubygems package, and the ruby-devel package.

SuSE:

    zypper install openmpi-devel libjson-c-devel hwloc-devel ruby-devel rubygems

RHEL:

    yum install openmpi-devel json-c-devel hwloc-devel ruby-devel rubygems

Alternatively these can be installed from source, and an alternate MPI
implementation to OpenMPI can be selected.  See

    ./configure --help

for details on how to use non-standard install locations for build
requirements through the

    ./configure --with-<feature>

options.


BUILD INSTRUCTIONS
------------------
To build all targets and install it in a "build/geopm" subdirectory of your
home directory run the following commands:

    ./autogen.sh
    ./configure --prefix=$HOME/build/geopm
    make
    make install

An RPM can be created on a RHEL or SuSE system with the

    make rpm

target.  Note that the --with-mpi-bin option may be required to inform
configure about the location of the MPI compiler wrappers.  The following
command may be sufficient to determine the location:

    dirname $(find /usr -name mpicc)

To build in an environment without support for OpenMP (i.e. clang on Mac OS X)
use the

    ./configure --disable-openmp

option.  The

    ./configure --disable-mpi

option can be used to build only targets which do not require MPI.  By default
MPI targets are built.


RUN REQUIREMENTS
----------------
We are targeting SLES12 and RHEL7 distributions for functional runtime
support.  The msr-safe kernel driver must be loaded at runtime to support user
level read and write of white-listed MSRs.  The source code for the driver can
be found here: <https://github.com/scalability-llnl/msr-safe>.

Alternately you can run geopm as root with the standard msr driver loaded:

    modprobe msr

Note that other Linux mechanisms for power management can interfere with
geopm, and these must be disabled.  We suggest the following:

    echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

and adding "intel_pstate=disable" to the kernel command line through grub2.


TESTING
-------
From within the source code directory, tests can be executed with the
"make check" target.  Some tests must be run as MPI applications, and
in order support these tests you must define the "MPIEXEC" environment
variable to whatever is appropriate for your your MPI runtime (default
is mpiexec).  The tests assume that this application will respect the
'-n' flag to set the number of MPI processes.  Note that '-n' is defined
by the MPI standard.  Each MPI dependent test is run with 16 MPI
processes.


STATUS
------
This software is pre-alpha quality and largely incomplete.  The software is
being provided for collaborative purposes.  The static modes are supported by
geopmpolicy and libgeopmpolicy.  The MPI runtime support of the geopmctl and
libgeopm is incomplete.

Test coverage is limited.  The line coverage results from gcov as reported by
gcovr for a recent build are below:

**GCC Code Coverage Report**

**Directories: src, plugin**

File                     |   Lines  |     Exec   |     Cover
------------------------ | -------- | ---------- | ---------
CircularBuffer.hpp       |   41     |     39     |     95%
Controller.cpp           |   238    |     156    |     66%
Decider.cpp              |   6      |     4      |     67%
Decider.hpp              |   1      |     1      |     100%
DeciderFactory.cpp       |   33     |     29     |     88%
Exception.cpp            |   98     |     68     |     70%
Exception.hpp            |   1      |     1      |     100%
GlobalPolicy.cpp         |   462    |     358    |     78%
HSXPlatformImp.cpp       |   208    |     149    |     72%
IVTPlatformImp.cpp       |   211    |     149    |     71%
LockingHashTable.hpp     |   157    |     138    |     88%
Observation.cpp          |   87     |     75     |     86%
Observation.hpp          |   2      |     2      |     100%
Platform.cpp             |   223    |     112    |     50%
PlatformFactory.cpp      |   83     |     64     |     77%
PlatformImp.cpp          |   109    |     87     |     80%
PlatformTopology.cpp     |   31     |     30     |     97%
Policy.cpp               |   125    |     125    |     100%
PolicyFlags.cpp          |   47     |     47     |     100%
Profile.cpp              |   397    |     294    |     74%
RAPLPlatform.cpp         |   87     |     80     |     92%
Region.cpp               |   233    |     221    |     95%
SampleRegulator.cpp      |   98     |     97     |     99%
SharedMemory.cpp         |   75     |     51     |     68%
TreeCommunicator.cpp     |   228    |     188    |     83%
geopm_ctl_spawn.c        |   2      |     0      |     0%
geopm_hash.c             |   13     |     13     |     100%
geopm_hash.h             |   2      |     2      |     100%
geopm_omp.c              |   11     |     0      |     0%
geopm_plugin.c           |   19     |     19     |     100%
geopm_pmpi.c             |   127    |     0      |     0%
geopm_policy_message.c   |   8      |     8      |     100%
geopm_time.h             |   16     |     16     |     100%
geopm_version.c          |   2      |     0      |     0%
BalancingDecider.cpp     |   66     |     61     |     92%
BalancingDecider.hpp     |   1      |     1      |     100%
GoverningDecider.cpp     |   64     |     60     |     94%
GoverningDecider.hpp     |   1      |     1      |     100%
TOTAL                    |   3613   |     2746   |     76%
